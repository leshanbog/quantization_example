{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas datasets ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speed evaluation\n",
    "\n",
    "We will benchmark the inference speed of a single Qwen-3 8B model on an NVIDIA L40 using vLLM. The model will be served in four precisions: BF16, FP8, AWQ INT4 (default kernel) and AWQ INT4 (Marlin kernel).\n",
    "\n",
    "We're going to measure three main metrics: time-to-first-token (TTFT), time-per-output-token (TPOT) and full end-to-end latency.\n",
    "\n",
    "Note: The default AWQ INT4 is the original method in vLLM, while the Marlin kernel is a newer, more optimized version designed for even better speed and efficiency on modern GPUs.\n",
    "\n",
    "Both AWQ (default) and AWQ Marlin use INT4 quantization to make the model smaller and faster. The default kernel is the original method for running AWQ quantized models in vLLM, while the Marlin kernel is a newer, more optimized version designed for even better speed and efficiency. Here, a “kernel” just means the low-level code that runs on the GPU to do the computation. For this lesson, you only need to know that Marlin aims to improve performance compared to the default approach. You can find more details about the Marlin kernel [here](https://github.com/IST-DASLab/marlin)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-22 14:33:10 [__init__.py:244] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from utils import setup_benchmark_environment, bench_single_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ok = setup_benchmark_environment()\n",
    "assert ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    {\n",
    "        \"name\": \"Qwen-3 8B / BF16\",\n",
    "        \"hf_id\": \"Qwen/Qwen3-8B\",\n",
    "        \"vllm_args\": [\n",
    "            \"--max-model-len\", \"2048\",\n",
    "            \"--max-seq-len-to-capture\", \"2048\",\n",
    "            \"--gpu-memory-utilization\", \"0.96\",\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Qwen-3 8B / FP8\",\n",
    "        \"hf_id\": \"Qwen/Qwen3-8B-FP8\",\n",
    "        \"vllm_args\": [\n",
    "            \"--max-model-len\", \"2048\",\n",
    "            \"--max-seq-len-to-capture\", \"2048\",\n",
    "            \"--gpu-memory-utilization\", \"0.96\",\n",
    "            \"--quantization\", \"fp8\",\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Qwen-3 8B / AWQ 4bit\",\n",
    "        \"hf_id\": \"Qwen/Qwen3-8B-AWQ\",\n",
    "        \"vllm_args\": [\n",
    "            \"--max-model-len\", \"2048\",\n",
    "            \"--max-seq-len-to-capture\", \"2048\",\n",
    "            \"--gpu-memory-utilization\", \"0.96\",\n",
    "            \"--quantization\", \"awq\",\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Qwen-3 8B / AWQ Marlin 4bit \",\n",
    "        \"hf_id\": \"Qwen/Qwen3-8B-AWQ\",\n",
    "        \"vllm_args\": [\n",
    "            \"--max-model-len\", \"2048\",\n",
    "            \"--max-seq-len-to-capture\", \"2048\",\n",
    "            \"--gpu-memory-utilization\", \"0.96\",\n",
    "            \"--dtype\", \"half\",\n",
    "            \"--quantization\", \"awq_marlin\",\n",
    "        ],\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d340dd194a4244a3a995e31502f2b87d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 14 files:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4a79f5d41bd4bde8615627e7b5034ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 11 files:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2f5f9ac0bbd48e389e23bc638d9582b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25d8815c18364c3d87836c32062a7d66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "for model in models:\n",
    "    snapshot_download(repo_id=model[\"hf_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.path.dirname(os.path.abspath(\"__file__\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for model in models:\n",
    "    res = bench_single_model(\n",
    "        model_name=model[\"hf_id\"],\n",
    "        port=9134,\n",
    "        request_rate=10,\n",
    "        num_prompts=200,\n",
    "        vllm_path=os.path.join(current_dir, \"vllm\"),\n",
    "        vllm_args=model[\"vllm_args\"],\n",
    "        input_len=512,\n",
    "        output_len=128\n",
    "    )\n",
    "    res[\"name\"] = model[\"name\"]\n",
    "    results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>throughput_tokens_per_sec</th>\n",
       "      <th>ttft_mean</th>\n",
       "      <th>ttft_p90</th>\n",
       "      <th>tpot_mean</th>\n",
       "      <th>tpot_p90</th>\n",
       "      <th>e2el_mean</th>\n",
       "      <th>e2el_p90</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Qwen-3 8B / BF16</td>\n",
       "      <td>1052.84</td>\n",
       "      <td>144.20</td>\n",
       "      <td>224.63</td>\n",
       "      <td>50.84</td>\n",
       "      <td>56.68</td>\n",
       "      <td>6243.78</td>\n",
       "      <td>7276.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Qwen-3 8B / FP8</td>\n",
       "      <td>1119.78</td>\n",
       "      <td>100.66</td>\n",
       "      <td>147.52</td>\n",
       "      <td>33.47</td>\n",
       "      <td>37.64</td>\n",
       "      <td>4109.30</td>\n",
       "      <td>4872.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Qwen-3 8B / AWQ 4bit</td>\n",
       "      <td>868.82</td>\n",
       "      <td>283.24</td>\n",
       "      <td>461.84</td>\n",
       "      <td>104.87</td>\n",
       "      <td>122.98</td>\n",
       "      <td>12778.73</td>\n",
       "      <td>15857.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Qwen-3 8B / AWQ Marlin 4bit</td>\n",
       "      <td>1170.49</td>\n",
       "      <td>99.31</td>\n",
       "      <td>160.09</td>\n",
       "      <td>24.91</td>\n",
       "      <td>29.16</td>\n",
       "      <td>3032.04</td>\n",
       "      <td>3705.31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           name  throughput_tokens_per_sec  ttft_mean  \\\n",
       "0              Qwen-3 8B / BF16                    1052.84     144.20   \n",
       "1               Qwen-3 8B / FP8                    1119.78     100.66   \n",
       "2          Qwen-3 8B / AWQ 4bit                     868.82     283.24   \n",
       "3  Qwen-3 8B / AWQ Marlin 4bit                     1170.49      99.31   \n",
       "\n",
       "   ttft_p90  tpot_mean  tpot_p90  e2el_mean  e2el_p90  \n",
       "0    224.63      50.84     56.68    6243.78   7276.14  \n",
       "1    147.52      33.47     37.64    4109.30   4872.23  \n",
       "2    461.84     104.87    122.98   12778.73  15857.04  \n",
       "3    160.09      24.91     29.16    3032.04   3705.31  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df[['name', 'throughput_tokens_per_sec', 'ttft_mean', 'ttft_p90', 'tpot_mean', 'tpot_p90', 'e2el_mean', 'e2el_p90']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📝 What These Results Show\n",
    "\n",
    "- FP8 cuts per-token latency by ~35% versus BF16 — Ada/Hopper GPUs (for example L40, H100, H200) include native FP8 tensor‑cores, so decoding cost drops from 50.8 ms per token (BF16) to 33.5 ms (FP8) while throughput rises.\n",
    "\n",
    "- AWQ‑Marlin wins overall — it uses an optimized kernel code for dramatically less memory movement and a mixed-precision dot-product the GPU can execute in one shot, giving it the best overall throughput (1170 tok/s) and the lowest TPOT (24.9 ms).\n",
    "\n",
    "- Kernel quality beats bit‑width.\n",
    "\n",
    "  - The default AWQ path in vLLM dequantises 4‑bit weights to FP16 every layer and every token, and promotes BF16 activations to FP16 before the matmul. The extra memory traffic and frequent dtype conversions wipe out most of the INT4 advantage.\n",
    "\n",
    "  - Marlin avoids this cost by expanding each weight block once per micro‑batch and re‑using it, keeping activations in FP16.\n",
    "\n",
    "  - INT4 alone does not guarantee speed — you need a kernel that is both compute‑ and memory‑efficient.\n",
    "\n",
    "- **Take‑aways for practitioners**\n",
    "\n",
    "  - Measure, don’t assume. Low precision brings speed-ups only when the kernel is memory-bandwidth-bound (i.e. the slow part is transferring data to and from GPU memory, not doing thecomputations) and when it avoids redundant conversions.\n",
    "\n",
    "  - Use FP8 on Ada/Hopper whenever memory allows — it delivers near‑FP16 quality with roughly one-third to one-half lower latency.\n",
    "\n",
    "  - Prefer AWQ‑Marlin when you need the smallest memory footprint and high throughput."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality evaluation\n",
    "\n",
    "In this part we will see how quantization affects quality.\n",
    "\n",
    "We will load several models, including a Llama-3.1-70B variant compressed with AWQ 4-bit, and measure their performance with `lm-eval`. Thanks to 4-bit quantization, the entire 70B checkpoint fits on a single NVIDIA L40 (46 GB), whereas the 16-bit original needs ≈140 GB and an 8-bit version ≈70 GB. \n",
    "\n",
    "We will also benchmark the Qwen3-8B, comparing its full-precision BF16 baseline with more compact FP8 and AWQ-INT4 variants.\n",
    "\n",
    "For Qwen-3 the authors already [report scores](https://huggingface.co/Qwen/Qwen3-8B-AWQ#performance) on tougher leaderboard-style benchmarks (LiveBench, GPQA, etc.), showing that AWQ-INT4 loses only slightly compared with BF16 while cutting memory use by ≳50 %.\n",
    "\n",
    "Below, the `evaluate_model` helper runs evaluation on HellaSwag, a multiple-choice commonsense reasoning benchmark designed to test a model's ability to choose the most plausible continuation of a given context. This allows you to observe the quality-vs-size trade-offs in a reproducible setup focused on everyday reasoning performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install lm-eval[vllm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-22 13:36:27 [__init__.py:244] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "from utils import evaluate_model\n",
    "\n",
    "models = [\n",
    "    {\n",
    "        \"name\": \"Llama 70B / GGUF 4bit\",\n",
    "        # repo that contains the *.gguf file\n",
    "        \"hf_id\": \"bartowski/Meta-Llama-3.1-70B-Instruct-GGUF\",\n",
    "        \"vllm_args\": [\n",
    "            \"--max-model-len\", \"256\",\n",
    "            \"--max-seq-len-to-capture\", \"256\",\n",
    "            \"--max-num-seqs\", \"1\",  # setting to 1 since evaluation requires logits that consume a lot of memory\n",
    "            \"--tokenizer\", \"meta-llama/Meta-Llama-3.1-70B-Instruct\"\n",
    "        ],\n",
    "    },    \n",
    "    {\n",
    "        \"name\": \"Llama 70b / AWQ 4bit\",\n",
    "        \"hf_id\": \"hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4\",\n",
    "        \"vllm_args\": [\n",
    "            \"--max-model-len\", \"256\",\n",
    "            \"--max-seq-len-to-capture\", \"256\",\n",
    "            \"--max-num-seqs\", \"1\",  # setting to 1 since evaluation requires logits that consume a lot of memory\n",
    "            \"--quantization\", \"awq_marlin\",\n",
    "        ],\n",
    "    },  \n",
    "    {\n",
    "        \"name\": \"Qwen-3 8B / BF16\",\n",
    "        \"hf_id\": \"Qwen/Qwen3-8B\",\n",
    "        \"vllm_args\": [\n",
    "            \"--max-model-len\", \"256\",\n",
    "            \"--max-seq-len-to-capture\", \"256\",\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Qwen-3 8B / FP8\",\n",
    "        \"hf_id\": \"Qwen/Qwen3-8B-FP8\",\n",
    "        \"vllm_args\": [\n",
    "            \"--max-model-len\", \"256\",\n",
    "            \"--max-seq-len-to-capture\", \"256\",\n",
    "            \"--quantization\", \"fp8\",\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Qwen-3 8B / AWQ 4bit\",\n",
    "        \"hf_id\": \"Qwen/Qwen3-8B-AWQ\",\n",
    "        \"vllm_args\": [\n",
    "            \"--max-model-len\", \"256\",\n",
    "            \"--max-seq-len-to-capture\", \"256\",\n",
    "            \"--quantization\", \"awq\",\n",
    "        ],\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "results = []\n",
    "\n",
    "for model in models:\n",
    "    hellaswag_acc = evaluate_model(\n",
    "        model_name=model[\"hf_id\"],\n",
    "        vllm_args=model[\"vllm_args\"],\n",
    "        limit=500,\n",
    "    )\n",
    "    results.append({\n",
    "        \"name\": model[\"name\"],\n",
    "        \"acc\": hellaswag_acc\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Llama 70b / AWQ 4bit</td>\n",
       "      <td>0.728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Qwen-3 8B / BF16</td>\n",
       "      <td>0.644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Qwen-3 8B / FP8</td>\n",
       "      <td>0.640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Qwen-3 8B / AWQ 4bit</td>\n",
       "      <td>0.634</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   name    acc\n",
       "0  Llama 70b / AWQ 4bit  0.728\n",
       "1      Qwen-3 8B / BF16  0.644\n",
       "2       Qwen-3 8B / FP8  0.640\n",
       "3  Qwen-3 8B / AWQ 4bit  0.634"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show that quantization introduces only a modest drop in accuracy while offering substantial memory savings.\n",
    "\n",
    "The Llama 70B model, even in its 4-bit AWQ form, achieves the highest HellaSwag accuracy beating all Qwen-3 8B variants. For Qwen-3 8B, the accuracy gently decreases from BF16 (64.4%) to FP8 (64.0%) and AWQ-4bit (63.4%), highlighting the typical quality-vs-efficiency trade-off.\n",
    "\n",
    "Overall, quantization proves effective for deploying high-performing models within limited hardware budgets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
